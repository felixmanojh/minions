# Minions Model Configuration
#
# Role-based model selection optimized for different tasks:
#   implementer - Code generation, feature implementation
#   reviewer    - Code review, bug detection
#   patcher     - FIM (fill-in-middle), surgical edits, diffs
#
# Presets by hardware:
#   nano   - 2GB RAM, ultra-light, basic tasks only
#   small  - 4GB RAM, fast, acceptable quality
#   medium - 8GB RAM, recommended balance (default)
#   large  - 16GB+ RAM, best quality
#
# Set preset via:
#   1. Environment: MINIONS_PRESET=medium
#   2. This file: preset: medium
#   3. Custom roles below (overrides preset)

preset: medium

# ─────────────────────────────────────────────────────────────
# Preset Definitions
# ─────────────────────────────────────────────────────────────
presets:
  # Ultra-light: Runs anywhere (Raspberry Pi, old laptops)
  nano:
    implementer:
      model: qwen2.5-coder:0.5b
      temperature: 0.2
      max_tokens: 256
    reviewer:
      model: qwen2.5-coder:0.5b
      temperature: 0.1
      max_tokens: 200
    patcher:
      model: starcoder2:3b
      temperature: 0.1
      max_tokens: 512

  # Fast local: Quick iterations, acceptable quality
  small:
    implementer:
      model: qwen2.5-coder:1.5b
      temperature: 0.2
      max_tokens: 512
    reviewer:
      model: deepseek-coder:1.3b
      temperature: 0.1
      max_tokens: 400
    patcher:
      model: starcoder2:3b
      temperature: 0.1
      max_tokens: 768

  # Recommended: Best balance of speed and quality
  medium:
    implementer:
      model: qwen2.5-coder:7b
      temperature: 0.2
      max_tokens: 1024
    reviewer:
      model: deepseek-coder:6.7b
      temperature: 0.1
      max_tokens: 800
    patcher:
      model: starcoder2:7b
      temperature: 0.1
      max_tokens: 1024

  # Best quality: For complex codebases
  large:
    implementer:
      model: qwen2.5-coder:14b
      temperature: 0.2
      max_tokens: 2048
    reviewer:
      model: deepseek-coder:33b
      temperature: 0.1
      max_tokens: 1500
    patcher:
      model: starcoder2:15b
      temperature: 0.1
      max_tokens: 2048

# ─────────────────────────────────────────────────────────────
# Role Specializations (why each model was chosen)
# ─────────────────────────────────────────────────────────────
#
# implementer: Qwen2.5-Coder
#   - Best overall coding model under 10B
#   - 92+ languages, near GPT-4o on benchmarks
#   - Strong at generation and reasoning
#
# reviewer: DeepSeek-Coder
#   - 300+ languages, great at finding bugs
#   - "Immediately usable" suggestions
#   - Good at explaining issues concisely
#
# patcher: StarCoder2 (FIM specialist)
#   - Trained specifically for fill-in-the-middle
#   - Best at surgical edits and partial rewrites
#   - 16K context for full-file + diff
#   - Uses <fim_prefix>, <fim_suffix>, <fim_middle> tokens
#
# ─────────────────────────────────────────────────────────────
# Alternative Models (language-specific or experimental)
# ─────────────────────────────────────────────────────────────
#
# Systems/Rust focused:
#   model: codegemma:7b
#
# Experimental (fast + reasoning):
#   model: smollm3:3b  # When available in Ollama
#
# Multimodal (images + code):
#   model: gemma3:4b
#
# ─────────────────────────────────────────────────────────────
# Custom Overrides (uncomment to override preset roles)
# ─────────────────────────────────────────────────────────────
# implementer:
#   model: codellama:7b-code
#   temperature: 0.2
#   max_tokens: 1024
#
# reviewer:
#   model: mistral:7b
#   temperature: 0.1
#   max_tokens: 800
#
# patcher:
#   model: starcoder2:7b
#   temperature: 0.1
#   max_tokens: 1024
